{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Text"
      ],
      "metadata": {
        "id": "rgoB2aUCgztQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Urban green spaces have become essential parts of city planning.\n",
        "Studies show that having parks and community gardens helps improve mental and\n",
        "physical health. In 2023, over 68% of city residents in the U.S. lived within a\n",
        "10-minute walk of a public green space. A recent report from the National Urban\n",
        "Landscape Association was shared via email (contact_us@green2030.org) and\n",
        "highlighted the need for biodiversity in city environments. Another contributor,\n",
        "michael92@ecochange.net, stated that urban ecosystems should be included in\n",
        "infrastructure planning to lessen the impact of climate change. As cities keep\n",
        "growing, sustainable urban design will be more important.\"\"\""
      ],
      "metadata": {
        "id": "GUPuxGe_iM3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What are regular expressions? Provide a pattern to extract emails containing both numbers and alphabets. Discuss the advantages and limitations of using regular expressions."
      ],
      "metadata": {
        "id": "kLKER-L7TcGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regular expressions are sequences of characters that matches a pattern in text. They are primarily used for matching and manipulating text as they are a powerful tool to find, validate and transform text data."
      ],
      "metadata": {
        "id": "baxkN0TSTzmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pattern to extract emails containing both numbers and alphabets.**"
      ],
      "metadata": {
        "id": "vSzfqPInUREj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "email_regex = r\"[a-zA-Z0-9]+@[a-zA-Z0-9]+\\.[a-zA-Z]{2,}\"\n",
        "\n",
        "#Assertion for non alphanumeric characters\n",
        "pattern = r\"(?<![\\w\\-\\.])\" + email_regex + r\"(?![\\w\\-\\.])\""
      ],
      "metadata": {
        "id": "g3YLBjE0UaZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The pattern above does not match email address containing periods, underscores or hyphen"
      ],
      "metadata": {
        "id": "y9mnxkq7XUaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "emails = re.findall(pattern, text)\n",
        "\n",
        "for email in emails:\n",
        "  print(email)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzHNYkZEXMgC",
        "outputId": "c60f2bac-a0eb-4511-a7dc-4fd0d319bf07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "michael92@ecochange.net\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages**\n",
        "\n",
        "\n",
        "*   Regular Expressions efficiently matches complex string patterns with minimal code\n",
        "*   Regular expressions are really powerful for text parsing and data validation tasks.\n",
        "*   Regex is supported across many programming languages and platforms\n",
        "\n"
      ],
      "metadata": {
        "id": "1jNwJ4aJa7YT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limitations**\n",
        "\n",
        "*   Regex patterns in raw text form are difficult to read and maintain.\n",
        "*   Regex gives poor performance on very large or nested inputs.\n",
        "*   Regex is not suitable for parsing hierarchical data structures like XML."
      ],
      "metadata": {
        "id": "SJ5NJ-fgbSxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What is the Bag of Words (BoW) technique? ● Explain how it differs from regular expressions and describe its limitations."
      ],
      "metadata": {
        "id": "5e1J3Vf-c8LQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words model is a text representation technique that converts a document or a string of text into an unordered collection or a 'bag' of words by counting word occurences or frequencies. It therefore disregards word order or grammar."
      ],
      "metadata": {
        "id": "tfSxaRLBdnji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code to employ BoW model in python is below:"
      ],
      "metadata": {
        "id": "AM0awByKvyHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary libraries and packages\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
      ],
      "metadata": {
        "id": "u2q4zI_bnIUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloads\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "infcs7ZuwTXX",
        "outputId": "9f43dfea-547a-4912-8124-50f3945bbf6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing:"
      ],
      "metadata": {
        "id": "lLWRzrOCv9xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pre-processing function that returns list of sentences with stopwords removed\n",
        "def preprocess(text):\n",
        "  sentences = sent_tokenize(text)\n",
        "  processed = list()\n",
        "\n",
        "  for sentence in sentences:\n",
        "    sentence = sentence.lower()\n",
        "    words = word_tokenize(sentence)\n",
        "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "    sentence = \" \".join(words)\n",
        "    processed.append(sentence)\n",
        "\n",
        "  return processed\n",
        "\n",
        "processed = preprocess(text)"
      ],
      "metadata": {
        "id": "_2xEkV5uwA7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize:"
      ],
      "metadata": {
        "id": "1s3ae2kJx_Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Bag of Words array\n",
        "bow = vectorizer.fit_transform(processed).toarray()\n",
        "print(bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0VaAIzOyBkc",
        "outputId": "72a4df9e-1cc8-465e-96fd-509de6daedd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0\n",
            "  1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0]\n",
            " [1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
            " [0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0\n",
            "  0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0]\n",
            " [0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            "  0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Difference between Regular Expressions and Bag of Words**"
      ],
      "metadata": {
        "id": "lUJ7gJRqe3WF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both Regular Expressions and Bag of Words are used for text processing, but serve different purposes and operate on different principles.\n",
        "\n",
        "*   Regular Expressions match text patterns whereas Bag of Words is used to represent word frequency in text numerically.\n",
        "*   Regular Expressions use symbols for pattern matching where BoW uses token frequency vectors.\n",
        "*   Regex is rule-based specific. Bag of Words, on the other hand, is general-purpose and statistical"
      ],
      "metadata": {
        "id": "atDAlWpqfBBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. What is TF-IDF (Term Frequency-Inverse Document Frequency)?\n",
        "# Explain the advantages of this approach and how it differs from regular expressions and the Bag of Words technique."
      ],
      "metadata": {
        "id": "BRqE0BCT5n5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF or Term Frequency-Inverse Document Frequency is a statistical measure used in information retrieval to evaluate the importance of a word in a document relative to a corpus. It combines Term Frequency (TF) with inverse document frequency (IDF) to reduce the weight of common terms and highlight distinctive words."
      ],
      "metadata": {
        "id": "LAL_ZE9I57Vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the implementation in python:"
      ],
      "metadata": {
        "id": "5qyaq0Bc6u8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "processed = preprocess(text)"
      ],
      "metadata": {
        "id": "Q0oKBWMP6xA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_vec = TfidfVectorizer() # TF_IDF Vectorizer object\n",
        "\n",
        "tfidf = tf_vec.fit_transform(processed).toarray() # TF_IDF Array\n",
        "print(tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SBBznx463zB",
        "outputId": "f2b8b194-728b-42c1-c04d-f231cb0292fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.         0.40238599\n",
            "  0.         0.         0.         0.27857682 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.40238599 0.         0.32996226 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.40238599 0.         0.32996226 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.40238599 0.         0.         0.         0.23871917 0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.31622777\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.31622777 0.         0.         0.\n",
            "  0.31622777 0.31622777 0.         0.         0.         0.31622777\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.31622777 0.         0.         0.         0.         0.\n",
            "  0.         0.31622777 0.         0.31622777 0.         0.\n",
            "  0.         0.         0.         0.         0.31622777 0.\n",
            "  0.         0.         0.31622777 0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.29945328 0.29945328 0.29945328 0.         0.         0.\n",
            "  0.         0.         0.         0.20731522 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.24555597 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.29945328\n",
            "  0.         0.         0.29945328 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.29945328\n",
            "  0.         0.         0.29945328 0.         0.         0.29945328\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.29945328 0.29945328]\n",
            " [0.         0.         0.         0.         0.25132886 0.\n",
            "  0.25132886 0.         0.         0.17399809 0.         0.\n",
            "  0.25132886 0.         0.         0.         0.         0.25132886\n",
            "  0.25132886 0.         0.         0.         0.25132886 0.\n",
            "  0.         0.         0.25132886 0.         0.         0.\n",
            "  0.         0.         0.         0.25132886 0.         0.\n",
            "  0.         0.         0.         0.25132886 0.25132886 0.\n",
            "  0.25132886 0.         0.         0.         0.         0.\n",
            "  0.25132886 0.25132886 0.         0.25132886 0.         0.\n",
            "  0.         0.         0.         0.         0.14910314 0.25132886\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.26702884 0.         0.\n",
            "  0.         0.26702884 0.         0.         0.26702884 0.\n",
            "  0.         0.26702884 0.         0.26702884 0.26702884 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.26702884 0.         0.\n",
            "  0.26702884 0.26702884 0.         0.         0.26702884 0.\n",
            "  0.         0.26702884 0.         0.         0.         0.26702884\n",
            "  0.         0.         0.         0.         0.21896747 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.26702884 0.         0.         0.1584173  0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.39677678 0.         0.         0.\n",
            "  0.         0.         0.39677678 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.39677678\n",
            "  0.         0.         0.         0.         0.39677678 0.\n",
            "  0.         0.         0.39677678 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.39677678 0.23539145 0.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages**\n",
        "*   TF_IDF emphasises important terms as it reduces the weight of frequent, non-informative words\n",
        "*   It is simple and efficient for text mining and retrieval tasks\n",
        "*   TF-IDF improves relevance in document ranking, search engines etc."
      ],
      "metadata": {
        "id": "zUxUoKjM7S71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differences among Regular Expressions, Bag of Words and TF-IDF**"
      ],
      "metadata": {
        "id": "-GhQTbJd8Lwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regular Expressions**:\n",
        "*   Pattern matching for identifying text structures like emails, dates, etc.\n",
        "*   Used for text preprocessing rather than feature extraction.\n",
        "*   No statistical or frequency-based representation of terms."
      ],
      "metadata": {
        "id": "cd9ONCsF8zAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words**:\n",
        "*   Represents text as a frequency count of words.\n",
        "*   Ignores grammar, word order or context.\n",
        "*   Simple and fast, but of no use when assessing importance of words."
      ],
      "metadata": {
        "id": "fAgxY2PB9LBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF_IDF**:\n",
        "*   Takes BoW one step further by weighting terms by rarity across documents\n",
        "*   Reduces the influence of less-informative but common words\n",
        "*   Provides more meaningful features for classification and retrieval"
      ],
      "metadata": {
        "id": "UbziUmAl9YJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. What are word embeddings? Describe the advantages of using word embeddings.\n"
      ],
      "metadata": {
        "id": "mOT5IzwAEGdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Embedding in Natural Language Processing represents a word as real-valued vectors that encodes semantic and syntactic relationships based on context. Words learnt from corpora are mapped to continuous vector spaces, enabling algorithms to capture similarities, analogies and syntactic structures. Popular models include Word2Vec, GloVe, FastText, etc."
      ],
      "metadata": {
        "id": "6_B68MzpEKLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gensim\n",
        "import gensim.models\n",
        "\n",
        "data = word_tokenize(text)\n",
        "model = gensim.models.Word2Vec([data], min_count=1) #Word to Vectors model\n",
        "model.wv.most_similar(['parks'], topn=10) #List of words related to 'parks'"
      ],
      "metadata": {
        "id": "oo5w874HMuw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d46cfba-c66b-4fd3-f997-e9fdcceebf1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('need', 0.23269957304000854),\n",
              " (')', 0.21507105231285095),\n",
              " ('city', 0.1953369677066803),\n",
              " ('should', 0.13609836995601654),\n",
              " ('2023', 0.120979905128479),\n",
              " ('for', 0.10494939237833023),\n",
              " ('health', 0.09511755406856537),\n",
              " ('of', 0.09415967017412186),\n",
              " ('have', 0.09364578872919083),\n",
              " ('green2030.org', 0.09035404026508331)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. What are stop words and how to remove them using the NLTK library?"
      ],
      "metadata": {
        "id": "4E2M1siAOwv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words are common words with minimal semantic value. They are deemed irrelevant for NLP tasks as they do not contribute significantly to the content's meaning. Filtering them out during text preprocessing for tasks like text classification, sentiment analysis or information retrieval reduce computational load and improve the efficiency of NLP models.\n",
        "\n",
        "Examples of stopwords are 'the', 'is', 'in', 'and', etc."
      ],
      "metadata": {
        "id": "FM5YI-R_Ozlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopword removal in python is carried out using the `stopwords` module of nltk.corpus package."
      ],
      "metadata": {
        "id": "MInW1nuBP8HM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#Tokenizing functions to extract tokens from the list\n",
        "def tokenize(text):\n",
        "  sentences = sent_tokenize(text)\n",
        "  tokens = list()\n",
        "\n",
        "  for sentence in sentences:\n",
        "    words = word_tokenize(sentence.lower())\n",
        "    tokens.extend(words)\n",
        "\n",
        "  return tokens\n",
        "\n",
        "\n",
        "# Set of stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenized text\n",
        "tokens = tokenize(text)\n",
        "\n",
        "#List of non-stopwords\n",
        "filtered = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GEI48mdQp5w",
        "outputId": "679ad165-7338-4b87-9052-97b3b453a9c7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is sentence tokenization and word tokenization?"
      ],
      "metadata": {
        "id": "THFDWHWHR9lO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence Tokenization**: Process of splitting a text into individual sentences. It involves identifying sentence boundaries, which can be challenging due to punctuation ambiguity. It is critical preprocessing step for tasks like parsing, machine translation, and sentiment analysis."
      ],
      "metadata": {
        "id": "U72VUrE1Um2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation in python is done using the sent_tokenize function from nltk.tokenize package in nltk library."
      ],
      "metadata": {
        "id": "m3M4ofX1VH4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "#Pre-trained tokenizer models to identify sentence boundaries correctly\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "sentences = sent_tokenize(text) # Tokenizer\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfR20_p5VKl4",
        "outputId": "8504a8bf-19ea-491a-db27-c4e2b6b7a61d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Urban green spaces have become essential parts of city planning.', 'Studies show that having parks and community gardens helps improve mental and\\nphysical health.', 'In 2023, over 68% of city residents in the U.S. lived within a\\n10-minute walk of a public green space.', 'A recent report from the National Urban\\nLandscape Association was shared via email (contact_us@green2030.org) and\\nhighlighted the need for biodiversity in city environments.', 'Another contributor,\\nmichael92@ecochange.net, stated that urban ecosystems should be included in\\ninfrastructure planning to lessen the impact of climate change.', 'As cities keep\\ngrowing, sustainable urban design will be more important.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Tokenization**: Process of splitting text into individual words or terms, which are the basic units of analysis. It enables text classification, sentiment analysis, and language modelling."
      ],
      "metadata": {
        "id": "SLrjcbTVWonV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#Pre-trained tokenizer models to identify sentence boundaries correctly\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "words = word_tokenize(text) # Tokenizer\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkczeZctYEnO",
        "outputId": "6ef83e9d-45b7-4265-e232-85341ae385e5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Urban', 'green', 'spaces', 'have', 'become', 'essential', 'parts', 'of', 'city', 'planning', '.', 'Studies', 'show', 'that', 'having', 'parks', 'and', 'community', 'gardens', 'helps', 'improve', 'mental', 'and', 'physical', 'health', '.', 'In', '2023', ',', 'over', '68', '%', 'of', 'city', 'residents', 'in', 'the', 'U.S.', 'lived', 'within', 'a', '10-minute', 'walk', 'of', 'a', 'public', 'green', 'space', '.', 'A', 'recent', 'report', 'from', 'the', 'National', 'Urban', 'Landscape', 'Association', 'was', 'shared', 'via', 'email', '(', 'contact_us', '@', 'green2030.org', ')', 'and', 'highlighted', 'the', 'need', 'for', 'biodiversity', 'in', 'city', 'environments', '.', 'Another', 'contributor', ',', 'michael92', '@', 'ecochange.net', ',', 'stated', 'that', 'urban', 'ecosystems', 'should', 'be', 'included', 'in', 'infrastructure', 'planning', 'to', 'lessen', 'the', 'impact', 'of', 'climate', 'change', '.', 'As', 'cities', 'keep', 'growing', ',', 'sustainable', 'urban', 'design', 'will', 'be', 'more', 'important', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}